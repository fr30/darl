defaults:
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: tpe

hydra:
  sweeper:
    sampler:
      seed: 2137
      consider_magic_clip: true
    direction: maximize
    study_name: minigrid
    storage: null
    n_trials: 100
    n_jobs: 1
    max_failure_rate: 0.0
    params:
      optim.policy_lr: range(1e-4, 1e-3, 1e-4)
      optim.q_lr: range(1e-4, 1e-3, 1e-4)
      optim.alpha_lr: range(1e-4, 1e-3, 1e-4)
      sac.alpha: range(0.2, 0.6, 0.1)
      sac.target_entropy_scale: choice(0.82, 0.89, 0.98)
      sac.tau: range(5e-3, 5e-2, 1e-3)
      env.grayscale: choice(False, True)
      env.frame_stack: choice(1, 3)

# Training metadata
meta:
  exp_name: 'test'
  seed: 1
  torch_deterministic: True
  cuda: True
  track: False
  wandb_project_name: "Darl"
  wandb_entity: ""

actor:
  hidden_features: 256

critic:
  hidden_features: 256

encoder:
  img_size: 84
  num_filters: 32
  out_features: 50
  crop: True

env:
  id: "InvertedPendulum-v4"
  grayscale: True
  capture_video: False

train:
  total_timesteps: 100_000
  buffer_size: 100_000
  n_envs: 1
  batch_size: 256

sac:
  gamma: 0.99
  learning_starts: 5e3
  update_frequency: 4
  policy_frequency: 8000
  autotune: True
  target_entropy_scale: 0.98
  alpha: 0.2
  tau: 0.035

optim:
  q_lr: 0.0006
  policy_lr: 0.0001
  alpha_lr: 0.001
